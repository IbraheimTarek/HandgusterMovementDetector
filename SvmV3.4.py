import cv2
import numpy as np
from sklearn import svm
from sklearn.model_selection import train_test_split
from skimage.feature import hog
import pyautogui

# Histogram of Oriented Gradients is particularly effective in capturing the shape and edge information in an image

def extract_hog_features(image):
    '''Function to Hima extract HOG features from an image'''
    if image.dtype != np.uint8:
        image = (image * 255).astype(np.uint8)

    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    features, _ = hog(gray, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', visualize=True)
    return features

def train_svm(X_train, y_train):
    '''Function to train an SVM classifier'''
    clf = svm.SVC(kernel='linear')
    clf.fit(X_train, y_train)
    return clf


def predict_hand_direction(svm_model, image):
    '''Function to predict hand direction using the trained SVM'''
    features = extract_hog_features(image)
    prediction = svm_model.predict([features])
    return prediction[0]

#sets the random seed to 42. Setting a random seed ensures that the random numbers generated by NumPy 
np.random.seed(42)
num_samples = 200

#This line defines the number of samples you want to generate for each hand direction (left, right, up, down). In this case, it's set to 200 samples.
X_left = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual left-hand images
X_right = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual right-hand images
X_up = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual up-hand images
X_down = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual down-hand images

#these arrays y_left, y_right, y_up, and y_down represent the labels corresponding
#  to the synthetic images generated for each hand direction. 
y_left = np.zeros(num_samples)
y_right = np.ones(num_samples)
y_up = np.full(num_samples, 2)
y_down = np.full(num_samples, 3)

# Combine samples for all directions
# The order of stacking corresponds to the order of images, 
# so the labels are correctly associated with their respective images.
X = np.vstack((X_left, X_right, X_up, X_down))
y = np.hstack((y_left, y_right, y_up, y_down))

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Extract HOG features for each image
X_train_features = np.array([extract_hog_features(img) for img in X_train])
X_test_features = np.array([extract_hog_features(img) for img in X_test])

# Train SVM classifier
svm_classifier = train_svm(X_train_features, y_train)

#       the trained SVM to predict hand direction in real-time (replace with actual video capture)
cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Resize the frame to match the training data size
    # DONT CHANGE ANYTHING IN TNIS LINE
    frame = cv2.resize(frame, (64, 64))

    # Predict hand direction
    direction = predict_hand_direction(svm_classifier, frame)

    # Print the predicted direction
    print(f"Predicted Direction: {direction}")

    # Translate predicted direction to arrow key press
    if direction == 0:
        pyautogui.press('left')
    elif direction == 1:
        pyautogui.press('right')
    elif direction == 2:
        pyautogui.press('up')
    elif direction == 3:
        pyautogui.press('down')

    cv2.imshow("Hand Movement Detection", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
