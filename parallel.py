import cv2
import numpy as np
from sklearn import svm
from sklearn.model_selection import train_test_split
from skimage.feature import hog
import pyautogui
import concurrent.futures
import threading
import queue
import time

image_queue = queue.Queue()

# Function to display images
def display_image():
    while True:
        image = image_queue.get()
        cv2.imshow("Hand Movement Detection", image)
        cv2.waitKey(1)  # Add a small delay to allow time for the display
# Function to detect hand contour using color segmentation and convex hull
def detect_hand_contour(image):
    # Convert image to 8-bit unsigned integer
    image = (image * 255).astype(np.uint8)

    # Filter the hand using color segmentation
    hand_mask = filter_hand_color(image)

    # Find contours in the hand mask
    contours, _ = cv2.findContours(hand_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Filter contours based on area
    valid_contours = [contour for contour in contours if cv2.contourArea(contour) > 1000]

    # Find the largest contour (assuming it's the hand)
    hand_contour = max(valid_contours, key=cv2.contourArea, default=None)

    # Convex hull to refine hand contour
    if hand_contour is not None and len(hand_contour) >= 3:
        hull = cv2.convexHull(hand_contour)
        hand_contour = hull

    return hand_contour

def filter_hand_color(image):
    '''Function to filter the hand using color segmentation'''
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    lower_skin = np.array([0, 20, 70], dtype=np.uint8)
    upper_skin = np.array([20, 255, 255], dtype=np.uint8)
    mask = cv2.inRange(hsv, lower_skin, upper_skin)
    return mask


# Function to process the image (HOG and hand contour detection)
def process_image(image):
    # Convert image to 8-bit unsigned integer
    image = (image * 255).astype(np.uint8)

    # Detect contours to find the hand
    hand_contour = detect_hand_contour(image)

    # Create a binary mask for the hand region
    hand_mask = np.zeros_like(image, dtype=np.uint8)
    if hand_contour is not None and len(hand_contour) >= 3:
        cv2.drawContours(hand_mask, [hand_contour], -1, (255, 255, 255), thickness=cv2.FILLED)
    segmented_hand = cv2.bitwise_and(image, hand_mask)

    # Extract HOG features from the segmented hand
    gray_hand = cv2.cvtColor(segmented_hand, cv2.COLOR_BGR2GRAY)
    hog_features, _ = hog(gray_hand, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', visualize=True)

    return hog_features, segmented_hand

def train_svm(X_train, y_train):
    '''Function to train an SVM classifier'''
    clf = svm.SVC(kernel='linear')
    clf.fit(X_train, y_train)
    return clf

def predict_hand_direction(svm_model, features):
    '''Function to predict hand direction using the trained SVM'''
    prediction = svm_model.predict([features])
    return prediction[0]

def pyautogui_thread(direction_queue):
    '''Thread to handle pyautogui operations'''
    time.sleep(1)  # Add a delay to give time for the other threads to start
    while True:
        direction = direction_queue.get()
        if direction is not None:
            if direction == 0:
                pyautogui.press('left')
            elif direction == 1:
                pyautogui.press('right')
            elif direction == 2:
                pyautogui.press('up')
            elif direction == 3:
                pyautogui.press('down')

# Set the random seed to 42. Setting a random seed ensures that the random numbers generated by NumPy 
np.random.seed(42)
num_samples = 200

# This line defines the number of samples you want to generate for each hand direction (left, right, up, down). In this case, it's set to 200 samples.
X_left = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual left-hand images
X_right = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual right-hand images
X_up = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual up-hand images
X_down = np.random.rand(num_samples, 64, 64, 3) * 255  # Replace with actual down-hand images

# These arrays y_left, y_right, y_up, and y_down represent the labels corresponding
# to the synthetic images generated for each hand direction. 
y_left = np.zeros(num_samples)
y_right = np.ones(num_samples)
y_up = np.full(num_samples, 2)
y_down = np.full(num_samples, 3)

# Combine samples for all directions
# The order of stacking corresponds to the order of images, 
# so the labels are correctly associated with their respective images.
X = np.vstack((X_left, X_right, X_up, X_down))
y = np.hstack((y_left, y_right, y_up, y_down))

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SVM classifier

X_train_features = np.array([process_image(img)[0] for img in X_train])
svm_classifier = train_svm(X_train_features, y_train)

cap = cv2.VideoCapture(0)
direction_queue = queue.Queue()
pyautogui_thread = threading.Thread(target=pyautogui_thread, args=(direction_queue,))
pyautogui_thread.start()


# Start the display thread
display_thread = threading.Thread(target=display_image)
display_thread.start()


while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.resize(frame, (64, 64))

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_image, frame)]
        results = [future.result() for future in concurrent.futures.as_completed(futures)]

    hog_features, segmented_hand = results[0]

    image_queue.put(segmented_hand)

    direction = predict_hand_direction(svm_classifier, hog_features)

    print(f"Predicted Direction: {direction}")

    direction_queue.put(direction)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
# while True:
#     ret, frame = cap.read()
#     if not ret:
#         break

#     # Resize the frame to match the training data size
#     frame = cv2.resize(frame, (64, 64))

#     # Process the image using concurrent.futures
#     with concurrent.futures.ThreadPoolExecutor() as executor:
#         futures = [executor.submit(process_image, frame)]
#         results = [future.result() for future in concurrent.futures.as_completed(futures)]

#     hog_features, original_image = results[0]

#     # Put the original image in the queue for display
#     image_queue.put(original_image)

#     # Predict hand direction
#     direction = predict_hand_direction(svm_classifier, hog_features)

#     # Print the predicted direction
#     print(f"Predicted Direction: {direction}")

#     # Put the direction in the queue for the pyautogui thread
#     direction_queue.put(direction)

#     if cv2.waitKey(1) & 0xFF == ord('q'):
#         break


cap.release()
cv2.destroyAllWindows()
